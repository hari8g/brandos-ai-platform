#!/usr/bin/env bash
set -e

PROJECT_ROOT="backend"
MLOPS_DIR="$PROJECT_ROOT/mlops"

echo "ðŸ¤– Setting up MLOps infrastructure for BrandOS AI Platform..."

cd "$MLOPS_DIR"

# Create training pipeline files
echo "ðŸ“š Creating training pipelines..."

# Formulation Generator Training
cat > training/formulation_generator/src/train.py << 'EOF'
"""
Formulation Generator Training Pipeline
"""
import mlflow
import mlflow.pytorch
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import pandas as pd
from sklearn.model_selection import train_test_split
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FormulationGenerator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(FormulationGenerator, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        return self.fc(lstm_out[:, -1, :])

def train_formulation_generator():
    """Train the formulation generator model."""
    mlflow.set_experiment("formulation_generator")
    
    with mlflow.start_run():
        # Model parameters
        input_size = 100
        hidden_size = 128
        output_size = 50
        learning_rate = 0.001
        epochs = 100
        
        # Log parameters
        mlflow.log_param("input_size", input_size)
        mlflow.log_param("hidden_size", hidden_size)
        mlflow.log_param("learning_rate", learning_rate)
        mlflow.log_param("epochs", epochs)
        
        # Initialize model
        model = FormulationGenerator(input_size, hidden_size, output_size)
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        
        # Training loop (placeholder)
        logger.info("Training formulation generator model...")
        
        # Log model
        mlflow.pytorch.log_model(model, "formulation_generator")
        
        logger.info("Training completed!")

if __name__ == "__main__":
    train_formulation_generator()
EOF

cat > training/formulation_generator/requirements.txt << 'EOF'
torch==2.1.1
mlflow==2.8.1
pandas==2.1.4
scikit-learn==1.3.2
numpy==1.24.3
EOF

cat > training/formulation_generator/Dockerfile << 'EOF'
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ ./src/
COPY config/ ./config/

CMD ["python", "src/train.py"]
EOF

# PH Predictor Training
cat > training/ph_predictor/src/train.py << 'EOF'
"""
PH Predictor Training Pipeline
"""
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def train_ph_predictor():
    """Train the PH prediction model."""
    mlflow.set_experiment("ph_predictor")
    
    with mlflow.start_run():
        # Model parameters
        n_estimators = 100
        max_depth = 10
        
        # Log parameters
        mlflow.log_param("n_estimators", n_estimators)
        mlflow.log_param("max_depth", max_depth)
        
        # Initialize model
        model = RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42
        )
        
        # Training (placeholder - replace with actual data)
        logger.info("Training PH predictor model...")
        
        # Log model
        mlflow.sklearn.log_model(model, "ph_predictor")
        
        logger.info("Training completed!")

if __name__ == "__main__":
    train_ph_predictor()
EOF

cat > training/ph_predictor/requirements.txt << 'EOF'
scikit-learn==1.3.2
mlflow==2.8.1
pandas==2.1.4
numpy==1.24.3
EOF

# Create serving services
echo "ðŸš€ Creating serving services..."

cat > serving/formulation_service/app.py << 'EOF'
"""
Formulation Service - FastAPI service for model serving
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import mlflow.pytorch
import torch
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Formulation Service")

class FormulationRequest(BaseModel):
    text: str
    category: str

class FormulationResponse(BaseModel):
    formulation: dict
    confidence: float

@app.post("/predict", response_model=FormulationResponse)
async def predict_formulation(request: FormulationRequest):
    """Generate formulation prediction."""
    try:
        # Load model from MLflow
        model = mlflow.pytorch.load_model("runs:/latest/formulation_generator")
        
        # Make prediction (placeholder)
        logger.info(f"Generating formulation for: {request.text}")
        
        return FormulationResponse(
            formulation={"ingredients": [], "cost": 0.0},
            confidence=0.95
        )
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
EOF

cat > serving/formulation_service/requirements.txt << 'EOF'
fastapi==0.104.1
uvicorn==0.24.0
mlflow==2.8.1
torch==2.1.1
pydantic==2.5.0
EOF

cat > serving/formulation_service/Dockerfile << 'EOF'
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY app.py .

EXPOSE 8001

CMD ["python", "app.py"]
EOF

# Create Airflow DAGs
echo "ðŸ”„ Creating Airflow DAGs..."

cat > pipelines/airflow_dags/train_models_dag.py << 'EOF'
"""
Airflow DAG for training all models
"""
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'brandos-ai',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'train_brandos_models',
    default_args=default_args,
    description='Train all BrandOS AI models',
    schedule_interval='@weekly',
    catchup=False
)

def train_formulation_generator():
    """Train formulation generator model."""
    print("Training formulation generator...")
    # Add actual training logic here

def train_ph_predictor():
    """Train PH predictor model."""
    print("Training PH predictor...")
    # Add actual training logic here

def deploy_models():
    """Deploy trained models to serving."""
    print("Deploying models...")
    # Add deployment logic here

# Define tasks
train_formulation = PythonOperator(
    task_id='train_formulation_generator',
    python_callable=train_formulation_generator,
    dag=dag
)

train_ph = PythonOperator(
    task_id='train_ph_predictor',
    python_callable=train_ph_predictor,
    dag=dag
)

deploy = PythonOperator(
    task_id='deploy_models',
    python_callable=deploy_models,
    dag=dag
)

# Define dependencies
train_formulation >> train_ph >> deploy
EOF

# Create Kubernetes manifests
echo "â˜¸ï¸ Creating Kubernetes manifests..."

mkdir -p pipelines/kubernetes

cat > pipelines/kubernetes/formulation-service.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: formulation-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: formulation-service
  template:
    metadata:
      labels:
        app: formulation-service
    spec:
      containers:
      - name: formulation-service
        image: brandos-ai/formulation-service:latest
        ports:
        - containerPort: 8001
        env:
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow-service:5000"
---
apiVersion: v1
kind: Service
metadata:
  name: formulation-service
spec:
  selector:
    app: formulation-service
  ports:
  - port: 8001
    targetPort: 8001
  type: ClusterIP
EOF

# Create MLflow configuration
echo "ðŸ“Š Setting up MLflow configuration..."

cat > mlflow/mlflow_config.yaml << 'EOF'
# MLflow tracking server configuration
backend_store_uri: sqlite:///mlflow.db
default_artifact_root: ./mlflow/artifacts
serve_artifacts: true
EOF

cat > mlflow/start_mlflow.sh << 'EOF'
#!/bin/bash
echo "ðŸš€ Starting MLflow tracking server..."
mlflow server \
    --backend-store-uri sqlite:///mlflow.db \
    --default-artifact-root ./mlflow/artifacts \
    --host 0.0.0.0 \
    --port 5000
EOF

chmod +x mlflow/start_mlflow.sh

# Create monitoring and logging
echo "ðŸ“ˆ Setting up monitoring..."

cat > utils/monitoring.py << 'EOF'
"""
Monitoring utilities for MLOps pipelines.
"""
import logging
from prometheus_client import Counter, Histogram, start_http_server
import time

# Prometheus metrics
PREDICTION_COUNTER = Counter('predictions_total', 'Total number of predictions')
PREDICTION_DURATION = Histogram('prediction_duration_seconds', 'Prediction duration')

def setup_monitoring(port=8000):
    """Setup Prometheus monitoring."""
    start_http_server(port)
    logging.info(f"Prometheus metrics server started on port {port}")

def log_prediction(model_name: str, duration: float, success: bool):
    """Log prediction metrics."""
    PREDICTION_COUNTER.labels(model=model_name, success=success).inc()
    PREDICTION_DURATION.labels(model=model_name).observe(duration)
EOF

# Create infrastructure as code
echo "ðŸ—ï¸ Creating infrastructure as code..."

mkdir -p infra/terraform

cat > infra/terraform/main.tf << 'EOF'
# Terraform configuration for BrandOS AI Platform infrastructure
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
}

# EKS Cluster for ML serving
resource "aws_eks_cluster" "brandos_ml" {
  name     = "brandos-ml-cluster"
  role_arn = aws_iam_role.eks_cluster.arn
  version  = "1.28"

  vpc_config {
    subnet_ids = var.subnet_ids
  }
}

# IAM role for EKS cluster
resource "aws_iam_role" "eks_cluster" {
  name = "brandos-eks-cluster-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "eks.amazonaws.com"
        }
      }
    ]
  })
}
EOF

cat > infra/terraform/variables.tf << 'EOF'
variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "us-west-2"
}

variable "subnet_ids" {
  description = "Subnet IDs for EKS cluster"
  type        = list(string)
}
EOF

# Create deployment scripts
echo "ðŸš€ Creating deployment scripts..."

cat > scripts/deploy_mlops.sh << 'EOF'
#!/bin/bash
echo "ðŸš€ Deploying MLOps infrastructure..."

# Start MLflow
echo "ðŸ“Š Starting MLflow..."
cd mlops/mlflow
./start_mlflow.sh &
MLFLOW_PID=$!

# Wait for MLflow to start
sleep 5

# Start training services
echo "ðŸ“š Starting training services..."
cd ../training/formulation_generator
docker build -t brandos-ai/formulation-generator .
docker run -d --name formulation-generator brandos-ai/formulation-generator

echo "âœ… MLOps infrastructure deployed!"
echo "ðŸ“Š MLflow: http://localhost:5000"
echo "ðŸ¤– Training services: Running in Docker"
EOF

chmod +x scripts/deploy_mlops.sh

# Create a comprehensive README
cat > README.md << 'EOF'
# BrandOS AI Platform - MLOps Infrastructure

This directory contains the complete MLOps infrastructure for the BrandOS AI Platform.

## ðŸ—ï¸ Architecture

```
mlops/
â”œâ”€â”€ training/           # Model training pipelines
â”‚   â”œâ”€â”€ formulation_generator/
â”‚   â”œâ”€â”€ ph_predictor/
â”‚   â”œâ”€â”€ ingredient_optimizer/
â”‚   â””â”€â”€ cost_estimator/
â”œâ”€â”€ serving/            # Model serving services
â”‚   â”œâ”€â”€ formulation_service/
â”‚   â”œâ”€â”€ optimization_service/
â”‚   â””â”€â”€ validation_service/
â”œâ”€â”€ pipelines/          # Orchestration
â”‚   â”œâ”€â”€ airflow_dags/
â”‚   â”œâ”€â”€ kubeflow/
â”‚   â””â”€â”€ kubernetes/
â”œâ”€â”€ mlflow/            # Experiment tracking
â”œâ”€â”€ utils/             # Shared utilities
â””â”€â”€ infra/             # Infrastructure as code
```

## ðŸš€ Quick Start

1. **Setup Environment**
   ```bash
   cd backend
   pip install -r requirements.txt
   ```

2. **Start MLflow**
   ```bash
   cd mlops/mlflow
   ./start_mlflow.sh
   ```

3. **Deploy MLOps Infrastructure**
   ```bash
   ./scripts/deploy_mlops.sh
   ```

4. **Train Models**
   ```bash
   cd mlops/training/formulation_generator
   python src/train.py
   ```

## ðŸ“Š Monitoring

- **MLflow**: http://localhost:5000
- **Prometheus**: http://localhost:8000/metrics
- **Grafana**: http://localhost:3000

## ðŸ”„ CI/CD Pipeline

The platform includes:
- Automated model training with Airflow
- Model versioning with MLflow
- Kubernetes deployment
- Monitoring with Prometheus/Grafana

## ðŸ“š Model Types

1. **Formulation Generator**: Generates product formulations
2. **PH Predictor**: Predicts pH levels
3. **Cost Estimator**: Estimates production costs
4. **Ingredient Optimizer**: Optimizes ingredient ratios

## ðŸ› ï¸ Development

- Use `./scripts/start_dev.sh` for development
- Use `./scripts/run_tests.sh` for testing
- Use `./scripts/setup_db.sh` for database setup
EOF

echo "âœ… MLOps infrastructure setup complete!"
echo ""
echo "ðŸ“‹ Next steps:"
echo "1. Start MLflow: cd mlops/mlflow && ./start_mlflow.sh"
echo "2. Deploy infrastructure: ./scripts/deploy_mlops.sh"
echo "3. Train models: cd mlops/training/formulation_generator && python src/train.py"
echo ""
echo "ðŸ¤– Your MLOps infrastructure is ready!" 